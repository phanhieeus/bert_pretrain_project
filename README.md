# BERT Pretraining with Hugging Face

This project focuses on pretraining a BERT model using Hugging Face's `transformers` and `datasets` libraries. The training process follows the Masked Language Modeling (MLM) objective on a large text dataset.

## ğŸ“‚ Project Structure
```
bert_pretrain_project/
â”‚â”€â”€ data/                         # Contains datasets
â”‚   â”œâ”€â”€ raw/                      # Raw downloaded data
â”‚   â”œâ”€â”€ processed/                 # Processed and tokenized data
â”‚â”€â”€ models/                        # Directory for trained models
â”‚   â”œâ”€â”€ bert-pretrained/           # Saved pretrained model
â”‚â”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ dataset.py                 # Data processing scripts
â”‚   â”œâ”€â”€ model.py                   # Model definition
â”‚   â”œâ”€â”€ train.py                   # Training script
â”‚   â”œâ”€â”€ evaluate.py                # Model evaluation
â”‚   â”œâ”€â”€ config.py                  # Configuration settings
â”‚â”€â”€ scripts/                        # Shell scripts for automation
â”‚   â”œâ”€â”€ run_training.sh             # Script to launch training
â”‚â”€â”€ notebooks/                      # Jupyter notebooks for data exploration
â”‚â”€â”€ requirements.txt                # Required dependencies
â”‚â”€â”€ README.md                       # Project documentation
```

## ğŸš€ Installation
Ensure you have Python 3.8+ and install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“œ Data Preparation
Download and preprocess the dataset:
```bash
python src/dataset.py
```

## ğŸ‹ï¸ Training the Model
To start pretraining the BERT model, run:
```bash
python src/train.py
```

## ğŸ“Š Evaluating the Model
Evaluate the pretrained model:
```bash
python src/evaluate.py
```

## ğŸ›  Configuration
Modify `src/config.py` to adjust hyperparameters such as batch size, learning rate, and number of epochs.

## ğŸ’¾ Saving and Loading the Model
The trained model is saved in `models/bert-pretrained/`. To load the model:
```python
from transformers import BertForMaskedLM
model = BertForMaskedLM.from_pretrained("models/bert-pretrained/")
```

## ğŸ“Œ Notes
- The dataset used is Wikipedia.
- You can adjust hyperparameters in `config.py`.

## ğŸ“œ License
This project is licensed under the MIT License.

